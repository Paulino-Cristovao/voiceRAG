# âš™ï¸ ConfiguraÃ§Ã£o do Sistema - Voice RAG

## ğŸ“‹ Modelos e ConfiguraÃ§Ãµes Atuais

### ğŸ¤– Modelos OpenAI

| Componente | Modelo | DimensÃµes/Capacidade |
|------------|--------|---------------------|
| **Embeddings** | `text-embedding-3-large` | 3072 dimensÃµes (melhor qualidade) |
| **Chat/LLM** | `gpt-4o-mini` | ConversaÃ§Ã£o e geraÃ§Ã£o de respostas |
| **STT** | `whisper-1` | TranscriÃ§Ã£o de voz (portuguÃªs) |
| **TTS** | `tts-1` | SÃ­ntese de voz |

### ğŸ“Š ConfiguraÃ§Ãµes RAG

| ParÃ¢metro | Valor | DescriÃ§Ã£o |
|-----------|-------|-----------|
| `TOP_K` | 5 | NÃºmero de chunks retornados do FAISS |
| `CHUNK_SIZE` | 400 tokens | Tamanho de cada chunk de texto |
| `CHUNK_OVERLAP` | 50 tokens | SobreposiÃ§Ã£o entre chunks |
| `TEMPERATURE` | 0.3 | Temperatura do LLM (mais determinÃ­stico) |
| `MAX_TOKENS` | 400 | MÃ¡ximo de tokens na resposta |

## ğŸ”§ Arquivo .env

Todas as configuraÃ§Ãµes sÃ£o lidas do arquivo `.env`:

```bash
# API Key (OBRIGATÃ“RIO)
OPENAI_API_KEY=sk-proj-...

# Modelos (Opcional - usa defaults se nÃ£o especificado)
EMBEDDING_MODEL=text-embedding-3-large
CHAT_MODEL=gpt-4o-mini
TTS_MODEL=tts-1
WHISPER_MODEL=whisper-1

# RAG (Opcional)
TOP_K=5
CHUNK_SIZE=400
CHUNK_OVERLAP=50
```

## ğŸ” Como Funciona o RAG

### 1. IngestÃ£o (ingest_pdfs.py)

```
PDFs/TXTs â†’ ExtraÃ§Ã£o de Texto â†’ Chunking
                                    â†“
                            Embeddings (3072-dim)
                                    â†“
                            FAISS Index (L2)
```

### 2. Consulta (app.py)

```
Pergunta do UsuÃ¡rio
        â†“
Embedding da Pergunta (3072-dim)
        â†“
Busca FAISS (L2 distance)
        â†“
Top 5 Chunks Mais Relevantes
        â†“
Contexto + InformaÃ§Ã£o Cliente â†’ LLM
        â†“
Resposta Personalizada
```

## ğŸ“ˆ Melhorias Implementadas

### âœ… Embedding Model Upgrade

**Antes:** `text-embedding-3-small` (1536 dimensÃµes)
**Agora:** `text-embedding-3-large` (3072 dimensÃµes)

**BenefÃ­cios:**
- âœ… Melhor qualidade de embeddings
- âœ… Busca semÃ¢ntica mais precisa
- âœ… Melhor compreensÃ£o de portuguÃªs
- âœ… Menos falsos positivos

### âœ… Retrieval Melhorado

**Antes:** Top-3 chunks
**Agora:** Top-5 chunks

**BenefÃ­cios:**
- âœ… Mais contexto para o LLM
- âœ… Respostas mais completas
- âœ… Melhor cobertura de informaÃ§Ã£o

### âœ… Prompt Engineering

**MudanÃ§as:**
- âœ… Prompt mais restritivo (USE APENAS BASE DE CONHECIMENTO)
- âœ… Temperatura reduzida (0.7 â†’ 0.3) = mais determinÃ­stico
- âœ… InstruÃ§Ãµes explÃ­citas para citar fontes
- âœ… Fallback claro quando informaÃ§Ã£o nÃ£o existe

### âœ… Logging e Debug

O sistema agora mostra nos logs:

```
ğŸ” Query: Como posso pagar a minha fatura?
ğŸ“š Retrieved 5 chunks from knowledge base
  [1] sample_support.txt (chunk 1): MOZAITELECOMUNICAÃ‡ÃƒO - GUIA DE APOIO...
  [2] sample_support.txt (chunk 2): Como Pagar a Sua Fatura...
  ...
ğŸ’¬ Response: Paulino, pode pagar a sua fatura de vÃ¡rias formas...
```

## ğŸ¯ Garantias de Qualidade

### 1. Usa APENAS Dados Fornecidos

O sistema foi configurado para:
- âœ… Responder APENAS com informaÃ§Ã£o da base de conhecimento
- âœ… Indicar claramente quando nÃ£o sabe algo
- âœ… NÃ£o inventar ou assumir informaÃ§Ãµes
- âœ… Citar fontes quando relevante

### 2. LÃª API Key do .env

```python
# app.py e ingest_pdfs.py
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY not found in .env file")
```

**VerificaÃ§Ã£o:**
```bash
# Ver logs ao iniciar
python app.py

# SaÃ­da:
ğŸ“‹ Configuration:
  - Embedding Model: text-embedding-3-large
  - Chat Model: gpt-4o-mini
  - Top K Results: 5
  - API Key: sk-proj-...q00A
```

### 3. Contexto Sempre IncluÃ­do

Cada resposta inclui:
- âœ… 5 chunks mais relevantes do FAISS
- âœ… InformaÃ§Ã£o do cliente (nome, plano, status)
- âœ… InstruÃ§Ãµes rÃ­gidas de uso apenas da base de conhecimento

## ğŸ§ª Como Testar

### Teste 1: Pergunta na Base de Conhecimento

```
Pergunta: "Como posso pagar a minha fatura?"
Esperado: Resposta com mÃ©todos de pagamento do guia
```

### Teste 2: Pergunta Fora da Base

```
Pergunta: "Qual Ã© a capital de FranÃ§a?"
Esperado: "{Nome}, nÃ£o tenho essa informaÃ§Ã£o na minha base de dados..."
```

### Teste 3: Verificar Logs

```bash
# Terminal do servidor mostra:
ğŸ” Query: Como posso pagar?
ğŸ“š Retrieved 5 chunks from knowledge base
  [1] sample_support.txt (chunk 1): ...
  [2] sample_support.txt (chunk 2): ...
ğŸ’¬ Response: ...
```

## ğŸ”„ Reconstruir Ãndice

Quando adicionar/modificar PDFs:

```bash
# Apagar Ã­ndice antigo
rm -rf data/index.faiss data/metadata.pkl

# Reconstruir com novo modelo
python ingest_pdfs.py

# Reiniciar servidor
python app.py
```

## ğŸ“Š ComparaÃ§Ã£o de Modelos

### text-embedding-3-small vs text-embedding-3-large

| MÃ©trica | Small | Large |
|---------|-------|-------|
| DimensÃµes | 1536 | 3072 |
| Qualidade | Boa | Excelente |
| Custo/1M tokens | $0.02 | $0.13 |
| Velocidade | RÃ¡pida | MÃ©dia |
| **RecomendaÃ§Ã£o** | ProduÃ§Ã£o grande escala | **Qualidade mÃ¡xima** âœ… |

Para este projeto, usamos **large** porque:
- âœ… Melhor compreensÃ£o de portuguÃªs
- âœ… Base pequena (8 chunks) = custo baixo
- âœ… Qualidade Ã© prioridade

## ğŸ†˜ Troubleshooting

### Respostas GenÃ©ricas?

1. Verificar se chunks corretos estÃ£o sendo recuperados (ver logs)
2. Aumentar `TOP_K` para 7-10
3. Reduzir `CHUNK_SIZE` para 300 (chunks menores = mais especÃ­ficos)

### InformaÃ§Ã£o Incorreta?

1. Verificar conteÃºdo dos PDFs em `pdfs/`
2. Reconstruir Ã­ndice: `python ingest_pdfs.py`
3. Verificar logs: chunks recuperados correspondem Ã  pergunta?

### Custo Alto?

1. Downgrade para `text-embedding-3-small`
2. Reduzir `TOP_K` para 3
3. Cache de embeddings (implementar)

---

**Ãšltima AtualizaÃ§Ã£o:** 2025-10-03
**VersÃ£o:** 2.0.0 (Upgrade para text-embedding-3-large)
